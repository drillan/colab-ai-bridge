{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-warn-conflicts \"colab-ai-bridge[langchain] @ git+https://github.com/drillan/colab-ai-bridge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使い方\n",
    "\n",
    "### 基本的な使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# モデルの作成（セットアップは自動で完了します）\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "# モデルの実行\n",
    "response = model.invoke(\"フランスの首都は？\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの選択\n",
    "\n",
    "利用可能なモデルを確認："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import ai\n",
    "\n",
    "ai.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特定のモデルを使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# Gemini 2.5 Flash Liteを使用\n",
    "model = ColabLangChainModel(\"google/gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 構造化出力\n",
    "\n",
    "LangChain の Messages API を使用して構造化されたプロンプトを作成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "\n",
    "class City(BaseModel):\n",
    "    name: str\n",
    "    country: str\n",
    "    population: int\n",
    "\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは構造化されたデータを JSON 形式で返すアシスタントです。\"),\n",
    "    HumanMessage(\n",
    "        content=f\"\"\"東京について、以下の JSON スキーマに従って情報を返してください：\n",
    "{City.model_json_schema()}\n",
    "\n",
    "JSON のみを返してください。\"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "city_data = json.loads(response.content)\n",
    "city = City(**city_data)\n",
    "print(f\"{city.name}, {city.country}, 人口: {city.population:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプル\n",
    "\n",
    "### サンプル 1: リスト型フィールドを持つ構造化出力\n",
    "\n",
    "リスト型フィールドを含むデータモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "class EmailAddress(BaseModel):\n",
    "    email: str = Field(pattern=r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n",
    "    domain: str\n",
    "\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは構造化されたデータを JSON 形式で返すアシスタントです。\"),\n",
    "    HumanMessage(\n",
    "        content=f\"\"\"example@gmail.com のメールアドレス情報を以下のスキーマで抽出してください：\n",
    "{EmailAddress.model_json_schema()}\n",
    "\n",
    "JSON のみを返してください。\"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "email_data = json.loads(response.content)\n",
    "email = EmailAddress(**email_data)\n",
    "print(f\"メール: {email.email}\")\n",
    "print(f\"ドメイン: {email.domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 2: システムプロンプト付きモデル\n",
    "\n",
    "役割を持たせたモデルで専門的な回答を得る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author_name: str\n",
    "    year: int\n",
    "    genres: list[str]\n",
    "\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは文学の専門家です。本の情報を JSON 形式で提供してください。\"),\n",
    "    HumanMessage(\n",
    "        content=f\"\"\"村上春樹の『ノルウェイの森』について、以下のスキーマで情報を返してください：\n",
    "{Book.model_json_schema()}\n",
    "\n",
    "JSON のみを返してください。\"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "book_data = json.loads(response.content)\n",
    "book = Book(**book_data)\n",
    "print(f\"タイトル: {book.title}\")\n",
    "print(f\"著者: {book.author_name}\")\n",
    "print(f\"出版年: {book.year}\")\n",
    "print(f\"ジャンル: {', '.join(book.genres)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 3: プロンプトテンプレートの使用\n",
    "\n",
    "LangChain のプロンプトテンプレートで動的な値を渡す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは日本史の専門家です。歴史的な事実を正確に、わかりやすく説明してください。\"),\n",
    "        (\"human\", \"{topic}について簡潔に説明してください\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "result = chain.invoke({\"topic\": \"関ヶ原の戦い\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 4: 温度パラメータの調整\n",
    "\n",
    "temperature を変更して創造的/正確な出力を制御"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from colab_ai_bridge.core.config import ModelConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserProfile:\n",
    "    name: str\n",
    "    age: int\n",
    "    interests: list[str]\n",
    "\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "user = UserProfile(name=\"太郎\", age=25, interests=[\"プログラミング\", \"機械学習\", \"読書\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーのプロフィールに基づいて、パーソナライズされた提案をしてください。\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"ユーザー情報:\n",
    "- 名前: {name}\n",
    "- 年齢: {age}\n",
    "- 興味: {interests}\n",
    "\n",
    "おすすめの学習リソースを3つ教えてください\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "result = chain.invoke(\n",
    "    {\"name\": user.name, \"age\": user.age, \"interests\": \", \".join(user.interests)}\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 5: 温度パラメータによる出力の違い\n",
    "\n",
    "異なる温度設定での出力比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# 低温度（正確な出力）\n",
    "config_precise = ModelConfig(temperature=0.3)\n",
    "model_precise = ColabLangChainModel(config=config_precise)\n",
    "\n",
    "# 高温度（創造的な出力）\n",
    "config_creative = ModelConfig(temperature=1.0)\n",
    "model_creative = ColabLangChainModel(config=config_creative)\n",
    "\n",
    "prompt = \"AIについて一文で説明してください\"\n",
    "\n",
    "print(\"【正確な出力 (temperature=0.3)】\")\n",
    "result1 = model_precise.invoke(prompt)\n",
    "print(result1.content)\n",
    "print()\n",
    "\n",
    "print(\"【創造的な出力 (temperature=1.0)】\")\n",
    "result2 = model_creative.invoke(prompt)\n",
    "print(result2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 6: ストリーミング出力\n",
    "\n",
    "リアルタイムでレスポンスをストリーミング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "print(\"ストリーミング出力:\")\n",
    "for chunk in model.stream(\"日本の四季について説明してください\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
