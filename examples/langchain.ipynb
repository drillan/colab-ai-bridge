{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-warn-conflicts \"colab-ai-bridge[langchain] @ git+https://github.com/drillan/colab-ai-bridge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使い方\n",
    "\n",
    "### 基本的な使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# モデルの作成（セットアップは自動で完了します）\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "# モデルの実行\n",
    "response = model.invoke(\"フランスの首都は？\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの選択\n",
    "\n",
    "利用可能なモデルを確認："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import ai\n",
    "\n",
    "ai.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特定のモデルを使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# Gemini 2.5 Flash Liteを使用\n",
    "model = ColabLangChainModel(\"google/gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプル\n",
    "\n",
    "### サンプル 1: LCEL - 複数チェーンの組み合わせ\n",
    "\n",
    "LangChain Expression Language (LCEL) を使った宣言的なチェーン構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "# 複数のチェーンを組み合わせる\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"以下のテキストを3文で要約してください：\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"以下の日本語を英語に翻訳してください：\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "# チェーン1: 要約\n",
    "summary_chain = summary_prompt | model | StrOutputParser()\n",
    "\n",
    "# チェーン2: 要約 → 翻訳\n",
    "translate_chain = {\"text\": summary_chain} | translate_prompt | model | StrOutputParser()\n",
    "\n",
    "# 実行\n",
    "text = \"\"\"\n",
    "LangChainは、言語モデルを使用したアプリケーションを構築するためのフレームワークです。\n",
    "プロンプトテンプレート、チェーン、エージェント、メモリなどの機能を提供し、\n",
    "複雑なAIアプリケーションを簡単に構築できます。\n",
    "LCELという宣言的な構文により、複数の処理を簡潔につなげることができます。\n",
    "\"\"\"\n",
    "\n",
    "result = translate_chain.invoke({\"text\": text})\n",
    "print(\"要約 → 翻訳結果:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 2: 会話履歴管理\n",
    "\n",
    "MessagesPlaceholder を使った文脈を考慮した会話"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "# 会話履歴を含むプロンプトテンプレート\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"あなたは親切なアシスタントです。会話の文脈を理解して応答してください。\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "# 会話履歴\n",
    "history = [\n",
    "    HumanMessage(content=\"私の名前は太郎です\"),\n",
    "    AIMessage(content=\"はじめまして、太郎さん。よろしくお願いします。\"),\n",
    "    HumanMessage(content=\"今日は良い天気ですね\"),\n",
    "    AIMessage(content=\"そうですね。良い天気で気持ちが良いですね。\"),\n",
    "]\n",
    "\n",
    "# 履歴を踏まえた質問\n",
    "result = chain.invoke({\"history\": history, \"input\": \"私の名前を覚えていますか？\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 3: プロンプトテンプレートの使用\n",
    "\n",
    "LangChain のプロンプトテンプレートで動的な値を渡す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from colab_ai_bridge.core.config import ModelConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserProfile:\n",
    "    name: str\n",
    "    age: int\n",
    "    interests: list[str]\n",
    "\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "user = UserProfile(\n",
    "    name=\"太郎\", age=25, interests=[\"プログラミング\", \"機械学習\", \"読書\"]\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ユーザーのプロフィールに基づいて、パーソナライズされた提案をしてください。\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"ユーザー情報:\n",
    "- 名前: {name}\n",
    "- 年齢: {age}\n",
    "- 興味: {interests}\n",
    "\n",
    "おすすめの学習リソースを3つ教えてください\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "result = chain.invoke(\n",
    "    {\"name\": user.name, \"age\": user.age, \"interests\": \", \".join(user.interests)}\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 4: バッチ処理\n",
    "\n",
    "複数の入力を一度に処理して効率化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは翻訳アシスタントです。\"),\n",
    "        (\"human\", \"{text}を英語に翻訳してください\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "# 複数の入力を一度に処理\n",
    "inputs = [\n",
    "    {\"text\": \"おはようございます\"},\n",
    "    {\"text\": \"ありがとうございます\"},\n",
    "    {\"text\": \"さようなら\"},\n",
    "]\n",
    "\n",
    "results = chain.batch(inputs)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 5: 温度パラメータによる出力の違い\n",
    "\n",
    "異なる温度設定での出力比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# 低温度（正確な出力）\n",
    "config_precise = ModelConfig(temperature=0.3)\n",
    "model_precise = ColabLangChainModel(config=config_precise)\n",
    "\n",
    "# 高温度（創造的な出力）\n",
    "config_creative = ModelConfig(temperature=1.0)\n",
    "model_creative = ColabLangChainModel(config=config_creative)\n",
    "\n",
    "prompt = \"AIについて一文で説明してください\"\n",
    "\n",
    "print(\"【正確な出力 (temperature=0.3)】\")\n",
    "result1 = model_precise.invoke(prompt)\n",
    "print(result1.content)\n",
    "print()\n",
    "\n",
    "print(\"【創造的な出力 (temperature=1.0)】\")\n",
    "result2 = model_creative.invoke(prompt)\n",
    "print(result2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 6: ストリーミング出力\n",
    "\n",
    "リアルタイムでレスポンスをストリーミング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "print(\"ストリーミング出力:\")\n",
    "for chunk in model.stream(\"日本の四季について説明してください\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
