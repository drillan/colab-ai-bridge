{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --no-warn-conflicts \"colab-ai-bridge[langchain] @ git+https://github.com/drillan/colab-ai-bridge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使い方\n",
    "\n",
    "### 基本的な使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# モデルの作成（セットアップは自動で完了します）\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "# モデルの実行\n",
    "response = model.invoke(\"フランスの首都は？\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの選択\n",
    "\n",
    "利用可能なモデルを確認："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import ai\n",
    "\n",
    "ai.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特定のモデルを使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# Gemini 2.5 Flash Liteを使用\n",
    "model = ColabLangChainModel(\"google/gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 構造化出力\n",
    "\n",
    "LangChain の Messages API を使用して構造化されたプロンプトを作成："
   ]
  },
  {
   "cell_type": "code",
   "source": "from pydantic import BaseModel\n\nclass City(BaseModel):\n    name: str\n    country: str\n    population: int\n\nmodel = ExtendedColabLangChainModel()\nstructured_model = model.with_structured_output(City)\n\nresult = structured_model.invoke(\"東京について教えて\")\nprint(f\"{result.name}, {result.country}, 人口: {result.population:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## サンプル\n\n### サンプル 1: LCEL - 複数チェーンの組み合わせ\n\nLangChain Expression Language (LCEL) を使った宣言的なチェーン構築"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from colab_ai_bridge.langchain import ColabLangChainModel\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nmodel = ColabLangChainModel()\n\n# 複数のチェーンを組み合わせる\nsummary_prompt = ChatPromptTemplate.from_template(\n    \"以下のテキストを3文で要約してください：\\n\\n{text}\"\n)\n\ntranslate_prompt = ChatPromptTemplate.from_template(\n    \"以下の日本語を英語に翻訳してください：\\n\\n{text}\"\n)\n\n# チェーン1: 要約\nsummary_chain = summary_prompt | model | StrOutputParser()\n\n# チェーン2: 要約 → 翻訳\ntranslate_chain = (\n    {\"text\": summary_chain}\n    | translate_prompt\n    | model\n    | StrOutputParser()\n)\n\n# 実行\ntext = \"\"\"\nLangChainは、言語モデルを使用したアプリケーションを構築するためのフレームワークです。\nプロンプトテンプレート、チェーン、エージェント、メモリなどの機能を提供し、\n複雑なAIアプリケーションを簡単に構築できます。\nLCELという宣言的な構文により、複数の処理を簡潔につなげることができます。\n\"\"\"\n\nresult = translate_chain.invoke({\"text\": text})\nprint(\"要約 → 翻訳結果:\")\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "source": "### サンプル 2: 会話履歴管理\n\nMessagesPlaceholder を使った文脈を考慮した会話",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from colab_ai_bridge.langchain import ColabLangChainModel\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import HumanMessage, AIMessage\n\nmodel = ColabLangChainModel()\n\n# 会話履歴を含むプロンプトテンプレート\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"あなたは親切なアシスタントです。会話の文脈を理解して応答してください。\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\n\nchain = prompt | model\n\n# 会話履歴\nhistory = [\n    HumanMessage(content=\"私の名前は太郎です\"),\n    AIMessage(content=\"はじめまして、太郎さん。よろしくお願いします。\"),\n    HumanMessage(content=\"今日は良い天気ですね\"),\n    AIMessage(content=\"そうですね。良い天気で気持ちが良いですね。\"),\n]\n\n# 履歴を踏まえた質問\nresult = chain.invoke({\"history\": history, \"input\": \"私の名前を覚えていますか？\"})\nprint(result.content)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "class EmailAddress(BaseModel):\n",
    "    email: str = Field(pattern=r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n",
    "    domain: str\n",
    "\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは構造化されたデータを JSON 形式で返すアシスタントです。\"),\n",
    "    HumanMessage(\n",
    "        content=f\"\"\"example@gmail.com のメールアドレス情報を以下のスキーマで抽出してください：\n",
    "{EmailAddress.model_json_schema()}\n",
    "\n",
    "JSON のみを返してください。\"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "email_data = json.loads(response.content)\n",
    "email = EmailAddress(**email_data)\n",
    "print(f\"メール: {email.email}\")\n",
    "print(f\"ドメイン: {email.domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "from pydantic import BaseModel\n\nclass Book(BaseModel):\n    title: str\n    author_name: str\n    year: int\n    genres: list[str]\n\nmodel = ExtendedColabLangChainModel()\nstructured_model = model.with_structured_output(Book)\n\nresult = structured_model.invoke(\"村上春樹の『ノルウェイの森』について教えて\")\nprint(f\"タイトル: {result.title}\")\nprint(f\"著者: {result.author_name}\")\nprint(f\"出版年: {result.year}\")\nprint(f\"ジャンル: {', '.join(result.genres)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author_name: str\n",
    "    year: int\n",
    "    genres: list[str]\n",
    "\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは文学の専門家です。本の情報を JSON 形式で提供してください。\"),\n",
    "    HumanMessage(\n",
    "        content=f\"\"\"村上春樹の『ノルウェイの森』について、以下のスキーマで情報を返してください：\n",
    "{Book.model_json_schema()}\n",
    "\n",
    "JSON のみを返してください。\"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "book_data = json.loads(response.content)\n",
    "book = Book(**book_data)\n",
    "print(f\"タイトル: {book.title}\")\n",
    "print(f\"著者: {book.author_name}\")\n",
    "print(f\"出版年: {book.year}\")\n",
    "print(f\"ジャンル: {', '.join(book.genres)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 3: プロンプトテンプレートの使用\n",
    "\n",
    "LangChain のプロンプトテンプレートで動的な値を渡す"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### サンプル 4: バッチ処理\n\n複数の入力を一度に処理して効率化"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from colab_ai_bridge.langchain import ColabLangChainModel\nfrom langchain_core.prompts import ChatPromptTemplate\n\nmodel = ColabLangChainModel()\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"あなたは翻訳アシスタントです。\"),\n    (\"human\", \"{text}を英語に翻訳してください\")\n])\n\nchain = prompt | model\n\n# 複数の入力を一度に処理\ninputs = [\n    {\"text\": \"おはようございます\"},\n    {\"text\": \"ありがとうございます\"},\n    {\"text\": \"さようなら\"},\n]\n\nresults = chain.batch(inputs)\nfor i, result in enumerate(results, 1):\n    print(f\"{i}. {result.content}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "from colab_ai_bridge.core.config import ModelConfig\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserProfile:\n",
    "    name: str\n",
    "    age: int\n",
    "    interests: list[str]\n",
    "\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "user = UserProfile(name=\"太郎\", age=25, interests=[\"プログラミング\", \"機械学習\", \"読書\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ユーザーのプロフィールに基づいて、パーソナライズされた提案をしてください。\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"ユーザー情報:\n",
    "- 名前: {name}\n",
    "- 年齢: {age}\n",
    "- 興味: {interests}\n",
    "\n",
    "おすすめの学習リソースを3つ教えてください\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "result = chain.invoke(\n",
    "    {\"name\": user.name, \"age\": user.age, \"interests\": \", \".join(user.interests)}\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 5: 温度パラメータによる出力の違い\n",
    "\n",
    "異なる温度設定での出力比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "# 低温度（正確な出力）\n",
    "config_precise = ModelConfig(temperature=0.3)\n",
    "model_precise = ColabLangChainModel(config=config_precise)\n",
    "\n",
    "# 高温度（創造的な出力）\n",
    "config_creative = ModelConfig(temperature=1.0)\n",
    "model_creative = ColabLangChainModel(config=config_creative)\n",
    "\n",
    "prompt = \"AIについて一文で説明してください\"\n",
    "\n",
    "print(\"【正確な出力 (temperature=0.3)】\")\n",
    "result1 = model_precise.invoke(prompt)\n",
    "print(result1.content)\n",
    "print()\n",
    "\n",
    "print(\"【創造的な出力 (temperature=1.0)】\")\n",
    "result2 = model_creative.invoke(prompt)\n",
    "print(result2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 6: ストリーミング出力\n",
    "\n",
    "リアルタイムでレスポンスをストリーミング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_ai_bridge.langchain import ColabLangChainModel\n",
    "\n",
    "model = ColabLangChainModel()\n",
    "\n",
    "print(\"ストリーミング出力:\")\n",
    "for chunk in model.stream(\"日本の四季について説明してください\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}