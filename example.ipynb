{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nvimx5CjUuo",
    "outputId": "f4498c47-d620-456f-fdbc-72a6eabba393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.9/401.9 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.4/357.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.5/367.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.1/223.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.4/228.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for colab-pydantic-ai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --no-warn-conflicts git+https://github.com/drillan/colab-pydantic-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使い方\n",
    "\n",
    "### 基本的な使い方\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVbtWrj6jWMl",
    "outputId": "022f5f49-44e3-46e7-8241-179b4ee93dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フランスの首都は**パリ**です。\n"
     ]
    }
   ],
   "source": [
    "from colab_pydantic_ai import ColabGeminiModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "# モデルの作成（セットアップは自動で完了します）\n",
    "model = ColabGeminiModel()\n",
    "agent = Agent(model)\n",
    "\n",
    "# Agentの実行\n",
    "result = agent.run_sync(\"フランスの首都は？\")\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの選択\n",
    "\n",
    "利用可能なモデルを確認：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0hz3rTfj6Fw",
    "outputId": "a8ca54e0-a347-4867-a289-2dd2a82c5c6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google/gemini-2.5-flash', 'google/gemini-2.5-flash-lite']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import ai\n",
    "\n",
    "ai.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特定のモデルを使用：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Q5B-WcWj_qJ"
   },
   "outputs": [],
   "source": [
    "from colab_pydantic_ai import ColabGeminiModel\n",
    "\n",
    "# Gemini 2.5 Flash Liteを使用\n",
    "model = ColabGeminiModel(\"google/gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 構造化出力\n",
    "\n",
    "Pydantic AI の型安全な機能を活用できます：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW_C29FGlPaj",
    "outputId": "471cf858-2cb7-43d7-de2c-497cb48d14d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo, Japan, 人口: 14,000,000\n"
     ]
    }
   ],
   "source": [
    "from colab_pydantic_ai import ColabGeminiModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class City(BaseModel):\n",
    "    name: str\n",
    "    country: str\n",
    "    population: int\n",
    "\n",
    "\n",
    "model = ColabGeminiModel()\n",
    "agent = Agent(model, output_type=City)\n",
    "\n",
    "result = agent.run_sync(\"東京について教えて\")\n",
    "city = result.output\n",
    "print(f\"{city.name}, {city.country}, 人口: {city.population:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp-w9VQ_nnOg"
   },
   "source": [
    "## サンプル\n",
    "\n",
    "### サンプル 1: リスト型フィールドを持つ構造化出力\n",
    "\n",
    "リスト型フィールドを含むデータモデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_pydantic_ai import ColabGeminiModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class EmailAddress(BaseModel):\n",
    "    email: str = Field(pattern=r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n",
    "    domain: str\n",
    "\n",
    "\n",
    "model = ColabGeminiModel()\n",
    "agent = Agent(model, output_type=EmailAddress, retries=3)\n",
    "\n",
    "# バリデーションエラーが発生した場合、自動的にリトライされる\n",
    "result = agent.run_sync(\"example@gmail.comのメールアドレス情報を抽出してください\")\n",
    "email = result.output\n",
    "print(f\"メール: {email.email}\")\n",
    "print(f\"ドメイン: {email.domain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 2: システムプロンプト付き Agent\n",
    "\n",
    "役割を持たせた Agent で専門的な回答を得る\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_pydantic_ai import ColabGeminiModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author_name: str\n",
    "    year: int\n",
    "    genres: list[str]\n",
    "\n",
    "\n",
    "model = ColabGeminiModel()\n",
    "agent = Agent(model, output_type=Book)\n",
    "\n",
    "result = agent.run_sync(\"村上春樹の『ノルウェイの森』について教えて\")\n",
    "book = result.output\n",
    "print(f\"タイトル: {book.title}\")\n",
    "print(f\"著者: {book.author_name}\")\n",
    "print(f\"出版年: {book.year}\")\n",
    "print(f\"ジャンル: {', '.join(book.genres)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 3: 依存関係注入（Dependency Injection）\n",
    "\n",
    "RunContext を使って動的な値を Agent に渡す\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_pydantic_ai import ColabGeminiModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "model = ColabGeminiModel()\n",
    "agent = Agent(\n",
    "    model,\n",
    "    system_prompt=\"あなたは日本史の専門家です。歴史的な事実を正確に、わかりやすく説明してください。\",\n",
    ")\n",
    "\n",
    "result = agent.run_sync(\"関ヶ原の戦いについて簡潔に説明してください\")\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 4: 温度パラメータの調整\n",
    "\n",
    "temperature を変更して創造的/正確な出力を制御\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_pydantic_ai import ColabGeminiModel\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserProfile:\n",
    "    name: str\n",
    "    age: int\n",
    "    interests: list[str]\n",
    "\n",
    "\n",
    "model = ColabGeminiModel()\n",
    "agent = Agent(\n",
    "    model,\n",
    "    deps_type=UserProfile,\n",
    "    system_prompt=\"ユーザーのプロフィールに基づいて、パーソナライズされた提案をしてください。\",\n",
    ")\n",
    "\n",
    "\n",
    "@agent.system_prompt\n",
    "def add_user_context(ctx: RunContext[UserProfile]) -> str:\n",
    "    return f\"\"\"\n",
    "    ユーザー情報:\n",
    "    - 名前: {ctx.deps.name}\n",
    "    - 年齢: {ctx.deps.age}\n",
    "    - 興味: {\", \".join(ctx.deps.interests)}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "user = UserProfile(\n",
    "    name=\"太郎\", age=25, interests=[\"プログラミング\", \"機械学習\", \"読書\"]\n",
    ")\n",
    "result = agent.run_sync(\"おすすめの学習リソースを3つ教えてください\", deps=user)\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル 5: エラーハンドリングとリトライ\n",
    "\n",
    "バリデーションエラー時の自動リトライ機能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colab_pydantic_ai import ColabGeminiModel\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.settings import ModelSettings\n",
    "\n",
    "# 低温度（正確な出力）\n",
    "model_precise = ColabGeminiModel(settings=ModelSettings(temperature=0.3))\n",
    "agent_precise = Agent(model_precise)\n",
    "\n",
    "# 高温度（創造的な出力）\n",
    "model_creative = ColabGeminiModel(settings=ModelSettings(temperature=1.0))\n",
    "agent_creative = Agent(model_creative)\n",
    "\n",
    "prompt = \"AIについて一文で説明してください\"\n",
    "\n",
    "print(\"【正確な出力 (temperature=0.3)】\")\n",
    "result1 = agent_precise.run_sync(prompt)\n",
    "print(result1.output)\n",
    "print()\n",
    "\n",
    "print(\"【創造的な出力 (temperature=1.0)】\")\n",
    "result2 = agent_creative.run_sync(prompt)\n",
    "print(result2.output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
